{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1428571428571428,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 2.0169084072113037,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 1.6279,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 2.6328396797180176,
      "learning_rate": 4.966666666666667e-05,
      "loss": 1.6936,
      "step": 20
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.4758315086364746,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 1.6155,
      "step": 30
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.641626000404358,
      "learning_rate": 4.933333333333334e-05,
      "loss": 1.1746,
      "step": 40
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.9738858938217163,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 1.1425,
      "step": 50
    },
    {
      "epoch": 0.03,
      "grad_norm": 2.0193848609924316,
      "learning_rate": 4.9e-05,
      "loss": 0.8784,
      "step": 60
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.5466198921203613,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.8278,
      "step": 70
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.9785875082015991,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.7901,
      "step": 80
    },
    {
      "epoch": 0.05,
      "grad_norm": 2.288144826889038,
      "learning_rate": 4.85e-05,
      "loss": 0.7869,
      "step": 90
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.987364649772644,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.6544,
      "step": 100
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.510972738265991,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.7391,
      "step": 110
    },
    {
      "epoch": 0.07,
      "grad_norm": 2.2730581760406494,
      "learning_rate": 4.8e-05,
      "loss": 0.5936,
      "step": 120
    },
    {
      "epoch": 0.07,
      "grad_norm": 3.0781590938568115,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.5926,
      "step": 130
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.738619089126587,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.6343,
      "step": 140
    },
    {
      "epoch": 0.09,
      "grad_norm": 2.7491724491119385,
      "learning_rate": 4.75e-05,
      "loss": 0.5775,
      "step": 150
    },
    {
      "epoch": 0.09,
      "grad_norm": 2.161702871322632,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.5592,
      "step": 160
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.852090835571289,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.597,
      "step": 170
    },
    {
      "epoch": 0.1,
      "grad_norm": 3.458094596862793,
      "learning_rate": 4.7e-05,
      "loss": 0.52,
      "step": 180
    },
    {
      "epoch": 0.11,
      "grad_norm": 3.0037567615509033,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.5128,
      "step": 190
    },
    {
      "epoch": 0.11,
      "grad_norm": 4.576301097869873,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.5221,
      "step": 200
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.137152671813965,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.5122,
      "step": 210
    },
    {
      "epoch": 0.13,
      "grad_norm": 2.722390651702881,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.4795,
      "step": 220
    },
    {
      "epoch": 0.13,
      "grad_norm": 3.628971815109253,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.6365,
      "step": 230
    },
    {
      "epoch": 0.14,
      "grad_norm": 3.9831230640411377,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.5571,
      "step": 240
    },
    {
      "epoch": 0.14,
      "grad_norm": 3.378110885620117,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.613,
      "step": 250
    },
    {
      "epoch": 0.15,
      "grad_norm": 3.8870129585266113,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.5144,
      "step": 260
    },
    {
      "epoch": 0.15,
      "grad_norm": 4.838214874267578,
      "learning_rate": 4.55e-05,
      "loss": 0.5031,
      "step": 270
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.2362992763519287,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.529,
      "step": 280
    },
    {
      "epoch": 0.17,
      "grad_norm": 2.859877347946167,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.5324,
      "step": 290
    },
    {
      "epoch": 0.17,
      "grad_norm": 4.535916328430176,
      "learning_rate": 4.5e-05,
      "loss": 0.4965,
      "step": 300
    },
    {
      "epoch": 0.18,
      "grad_norm": 4.2276740074157715,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.5274,
      "step": 310
    },
    {
      "epoch": 0.18,
      "grad_norm": 7.005105018615723,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.5901,
      "step": 320
    },
    {
      "epoch": 0.19,
      "grad_norm": 3.983276844024658,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.6091,
      "step": 330
    },
    {
      "epoch": 0.19,
      "grad_norm": 5.196242332458496,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.5042,
      "step": 340
    },
    {
      "epoch": 0.2,
      "grad_norm": 3.927157163619995,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.5717,
      "step": 350
    },
    {
      "epoch": 0.21,
      "grad_norm": 3.029127836227417,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4671,
      "step": 360
    },
    {
      "epoch": 0.21,
      "grad_norm": 3.3520007133483887,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.4737,
      "step": 370
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.8666696548461914,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.6021,
      "step": 380
    },
    {
      "epoch": 0.22,
      "grad_norm": 3.936685562133789,
      "learning_rate": 4.35e-05,
      "loss": 0.5428,
      "step": 390
    },
    {
      "epoch": 0.23,
      "grad_norm": 2.8517816066741943,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.5493,
      "step": 400
    },
    {
      "epoch": 0.23,
      "grad_norm": 3.883310079574585,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.5655,
      "step": 410
    },
    {
      "epoch": 0.24,
      "grad_norm": 4.221883296966553,
      "learning_rate": 4.3e-05,
      "loss": 0.4878,
      "step": 420
    },
    {
      "epoch": 0.25,
      "grad_norm": 2.273014545440674,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.4293,
      "step": 430
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.801723003387451,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.4476,
      "step": 440
    },
    {
      "epoch": 0.26,
      "grad_norm": 3.3629326820373535,
      "learning_rate": 4.25e-05,
      "loss": 0.5235,
      "step": 450
    },
    {
      "epoch": 0.26,
      "grad_norm": 4.73838472366333,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.4842,
      "step": 460
    },
    {
      "epoch": 0.27,
      "grad_norm": 3.466411590576172,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.4778,
      "step": 470
    },
    {
      "epoch": 0.27,
      "grad_norm": 4.386445045471191,
      "learning_rate": 4.2e-05,
      "loss": 0.5115,
      "step": 480
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.397928237915039,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.4688,
      "step": 490
    },
    {
      "epoch": 0.29,
      "grad_norm": 2.569908857345581,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.4193,
      "step": 500
    },
    {
      "epoch": 0.29,
      "eval_bleu-4": 0.5853042310021457,
      "eval_rouge-1": 77.139434,
      "eval_rouge-2": 57.489216,
      "eval_rouge-l": 67.66379,
      "eval_runtime": 10.631,
      "eval_samples_per_second": 4.703,
      "eval_steps_per_second": 0.376,
      "step": 500
    },
    {
      "epoch": 0.29,
      "grad_norm": 4.494927406311035,
      "learning_rate": 4.15e-05,
      "loss": 0.5195,
      "step": 510
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.799308776855469,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.5718,
      "step": 520
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.943694829940796,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.4818,
      "step": 530
    },
    {
      "epoch": 0.31,
      "grad_norm": 4.568515300750732,
      "learning_rate": 4.1e-05,
      "loss": 0.4783,
      "step": 540
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.6812849044799805,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.4687,
      "step": 550
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.2219204902648926,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.4646,
      "step": 560
    },
    {
      "epoch": 0.33,
      "grad_norm": 6.342565536499023,
      "learning_rate": 4.05e-05,
      "loss": 0.5034,
      "step": 570
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.9393723011016846,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.507,
      "step": 580
    },
    {
      "epoch": 0.34,
      "grad_norm": 4.0366973876953125,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.5097,
      "step": 590
    },
    {
      "epoch": 0.34,
      "grad_norm": 2.6105451583862305,
      "learning_rate": 4e-05,
      "loss": 0.4762,
      "step": 600
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.445600748062134,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.4423,
      "step": 610
    },
    {
      "epoch": 0.35,
      "grad_norm": 3.919020891189575,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.5054,
      "step": 620
    },
    {
      "epoch": 0.36,
      "grad_norm": 5.369697093963623,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.4849,
      "step": 630
    },
    {
      "epoch": 0.37,
      "grad_norm": 3.5712780952453613,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.5003,
      "step": 640
    },
    {
      "epoch": 0.37,
      "grad_norm": 7.440522193908691,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.4677,
      "step": 650
    },
    {
      "epoch": 0.38,
      "grad_norm": 5.105062484741211,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.4798,
      "step": 660
    },
    {
      "epoch": 0.38,
      "grad_norm": 4.422175884246826,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.5072,
      "step": 670
    },
    {
      "epoch": 0.39,
      "grad_norm": 6.370905876159668,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.4397,
      "step": 680
    },
    {
      "epoch": 0.39,
      "grad_norm": 6.7642974853515625,
      "learning_rate": 3.85e-05,
      "loss": 0.5386,
      "step": 690
    },
    {
      "epoch": 0.4,
      "grad_norm": 4.327710151672363,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.5348,
      "step": 700
    },
    {
      "epoch": 0.41,
      "grad_norm": 4.252569198608398,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.4684,
      "step": 710
    },
    {
      "epoch": 0.41,
      "grad_norm": 5.304670810699463,
      "learning_rate": 3.8e-05,
      "loss": 0.4613,
      "step": 720
    },
    {
      "epoch": 0.42,
      "grad_norm": 4.791554927825928,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.3909,
      "step": 730
    },
    {
      "epoch": 0.42,
      "grad_norm": 4.638645648956299,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.521,
      "step": 740
    },
    {
      "epoch": 0.43,
      "grad_norm": 2.656277656555176,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.4875,
      "step": 750
    },
    {
      "epoch": 0.43,
      "grad_norm": 3.5451812744140625,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.5351,
      "step": 760
    },
    {
      "epoch": 0.44,
      "grad_norm": 3.9140000343322754,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.4328,
      "step": 770
    },
    {
      "epoch": 0.45,
      "grad_norm": 4.809997081756592,
      "learning_rate": 3.7e-05,
      "loss": 0.4986,
      "step": 780
    },
    {
      "epoch": 0.45,
      "grad_norm": 4.952734470367432,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.4373,
      "step": 790
    },
    {
      "epoch": 0.46,
      "grad_norm": 7.586422443389893,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.5656,
      "step": 800
    },
    {
      "epoch": 0.46,
      "grad_norm": 4.472262859344482,
      "learning_rate": 3.65e-05,
      "loss": 0.473,
      "step": 810
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.272454261779785,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.4853,
      "step": 820
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.8261845111846924,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.4149,
      "step": 830
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.5734353065490723,
      "learning_rate": 3.6e-05,
      "loss": 0.5162,
      "step": 840
    },
    {
      "epoch": 0.49,
      "grad_norm": 3.1485838890075684,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.3762,
      "step": 850
    },
    {
      "epoch": 0.49,
      "grad_norm": 2.512326240539551,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.4734,
      "step": 860
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.000740051269531,
      "learning_rate": 3.55e-05,
      "loss": 0.3806,
      "step": 870
    },
    {
      "epoch": 0.5,
      "grad_norm": 4.220278739929199,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.4284,
      "step": 880
    },
    {
      "epoch": 0.51,
      "grad_norm": 4.780506134033203,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.3833,
      "step": 890
    },
    {
      "epoch": 0.51,
      "grad_norm": 6.483672142028809,
      "learning_rate": 3.5e-05,
      "loss": 0.4453,
      "step": 900
    },
    {
      "epoch": 0.52,
      "grad_norm": 4.00534725189209,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.5014,
      "step": 910
    },
    {
      "epoch": 0.53,
      "grad_norm": 2.909200429916382,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.4832,
      "step": 920
    },
    {
      "epoch": 0.53,
      "grad_norm": 4.855235576629639,
      "learning_rate": 3.45e-05,
      "loss": 0.4981,
      "step": 930
    },
    {
      "epoch": 0.54,
      "grad_norm": 5.025314807891846,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.5149,
      "step": 940
    },
    {
      "epoch": 0.54,
      "grad_norm": 4.337216854095459,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.4673,
      "step": 950
    },
    {
      "epoch": 0.55,
      "grad_norm": 4.911008834838867,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.381,
      "step": 960
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.9941859245300293,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.434,
      "step": 970
    },
    {
      "epoch": 0.56,
      "grad_norm": 5.269810676574707,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.4273,
      "step": 980
    },
    {
      "epoch": 0.57,
      "grad_norm": 5.683711528778076,
      "learning_rate": 3.35e-05,
      "loss": 0.4308,
      "step": 990
    },
    {
      "epoch": 0.57,
      "grad_norm": 2.907745599746704,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.4473,
      "step": 1000
    },
    {
      "epoch": 0.57,
      "eval_bleu-4": 0.5440819094320097,
      "eval_rouge-1": 75.381564,
      "eval_rouge-2": 53.97977,
      "eval_rouge-l": 66.018644,
      "eval_runtime": 8.8873,
      "eval_samples_per_second": 5.626,
      "eval_steps_per_second": 0.45,
      "step": 1000
    },
    {
      "epoch": 0.58,
      "grad_norm": 4.433446407318115,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.369,
      "step": 1010
    },
    {
      "epoch": 0.58,
      "grad_norm": 4.823690891265869,
      "learning_rate": 3.3e-05,
      "loss": 0.4702,
      "step": 1020
    },
    {
      "epoch": 0.59,
      "grad_norm": 4.213583946228027,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.4569,
      "step": 1030
    },
    {
      "epoch": 0.59,
      "grad_norm": 5.71601676940918,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.4675,
      "step": 1040
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.7091126441955566,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.3578,
      "step": 1050
    },
    {
      "epoch": 0.61,
      "grad_norm": 3.2962734699249268,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.4,
      "step": 1060
    },
    {
      "epoch": 0.61,
      "grad_norm": 3.1862683296203613,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.4016,
      "step": 1070
    },
    {
      "epoch": 0.62,
      "grad_norm": 4.064300060272217,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.3606,
      "step": 1080
    },
    {
      "epoch": 0.62,
      "grad_norm": 5.789810657501221,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.4556,
      "step": 1090
    },
    {
      "epoch": 0.63,
      "grad_norm": 5.19739294052124,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.4286,
      "step": 1100
    },
    {
      "epoch": 0.63,
      "grad_norm": 7.2281341552734375,
      "learning_rate": 3.15e-05,
      "loss": 0.471,
      "step": 1110
    },
    {
      "epoch": 0.64,
      "grad_norm": 4.170691967010498,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.4256,
      "step": 1120
    },
    {
      "epoch": 0.65,
      "grad_norm": 4.5279221534729,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.4693,
      "step": 1130
    },
    {
      "epoch": 0.65,
      "grad_norm": 3.8934743404388428,
      "learning_rate": 3.1e-05,
      "loss": 0.4035,
      "step": 1140
    },
    {
      "epoch": 0.66,
      "grad_norm": 2.953181743621826,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.4462,
      "step": 1150
    },
    {
      "epoch": 0.66,
      "grad_norm": 7.27907657623291,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.4576,
      "step": 1160
    },
    {
      "epoch": 0.67,
      "grad_norm": 6.185770034790039,
      "learning_rate": 3.05e-05,
      "loss": 0.4704,
      "step": 1170
    },
    {
      "epoch": 0.67,
      "grad_norm": 8.512164115905762,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.5232,
      "step": 1180
    },
    {
      "epoch": 0.68,
      "grad_norm": 3.823105573654175,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.3628,
      "step": 1190
    },
    {
      "epoch": 0.69,
      "grad_norm": 4.426682949066162,
      "learning_rate": 3e-05,
      "loss": 0.471,
      "step": 1200
    },
    {
      "epoch": 0.69,
      "grad_norm": 5.04055118560791,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.368,
      "step": 1210
    },
    {
      "epoch": 0.7,
      "grad_norm": 5.251063823699951,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.4171,
      "step": 1220
    },
    {
      "epoch": 0.7,
      "grad_norm": 6.810103416442871,
      "learning_rate": 2.95e-05,
      "loss": 0.5062,
      "step": 1230
    },
    {
      "epoch": 0.71,
      "grad_norm": 6.681380748748779,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.5382,
      "step": 1240
    },
    {
      "epoch": 0.71,
      "grad_norm": 3.2635388374328613,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.5017,
      "step": 1250
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.906548500061035,
      "learning_rate": 2.9e-05,
      "loss": 0.4593,
      "step": 1260
    },
    {
      "epoch": 0.73,
      "grad_norm": 4.505155563354492,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.3847,
      "step": 1270
    },
    {
      "epoch": 0.73,
      "grad_norm": 5.970576286315918,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.416,
      "step": 1280
    },
    {
      "epoch": 0.74,
      "grad_norm": 3.7427027225494385,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.459,
      "step": 1290
    },
    {
      "epoch": 0.74,
      "grad_norm": 5.716041564941406,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.4237,
      "step": 1300
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.539072513580322,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.4264,
      "step": 1310
    },
    {
      "epoch": 0.75,
      "grad_norm": 6.497617244720459,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.4426,
      "step": 1320
    },
    {
      "epoch": 0.76,
      "grad_norm": 5.391618251800537,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.4258,
      "step": 1330
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.323307514190674,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.4079,
      "step": 1340
    },
    {
      "epoch": 0.77,
      "grad_norm": 5.073058605194092,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.5204,
      "step": 1350
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.187364339828491,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.4439,
      "step": 1360
    },
    {
      "epoch": 0.78,
      "grad_norm": 3.3659427165985107,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.3708,
      "step": 1370
    },
    {
      "epoch": 0.79,
      "grad_norm": 3.308227777481079,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.4073,
      "step": 1380
    },
    {
      "epoch": 0.79,
      "grad_norm": 6.05068826675415,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.4221,
      "step": 1390
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.439957141876221,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.392,
      "step": 1400
    },
    {
      "epoch": 0.81,
      "grad_norm": 5.3434882164001465,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.4833,
      "step": 1410
    },
    {
      "epoch": 0.81,
      "grad_norm": 7.620787620544434,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.3913,
      "step": 1420
    },
    {
      "epoch": 0.82,
      "grad_norm": 5.867905139923096,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.4125,
      "step": 1430
    },
    {
      "epoch": 0.82,
      "grad_norm": 5.93436861038208,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4109,
      "step": 1440
    },
    {
      "epoch": 0.83,
      "grad_norm": 4.850036144256592,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.4664,
      "step": 1450
    },
    {
      "epoch": 0.83,
      "grad_norm": 7.61714506149292,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.4183,
      "step": 1460
    },
    {
      "epoch": 0.84,
      "grad_norm": 6.071924686431885,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.4149,
      "step": 1470
    },
    {
      "epoch": 0.85,
      "grad_norm": 3.9589853286743164,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.3712,
      "step": 1480
    },
    {
      "epoch": 0.85,
      "grad_norm": 3.509108304977417,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.3982,
      "step": 1490
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.081723690032959,
      "learning_rate": 2.5e-05,
      "loss": 0.3548,
      "step": 1500
    },
    {
      "epoch": 0.86,
      "eval_bleu-4": 0.5813774318070926,
      "eval_rouge-1": 77.94425399999999,
      "eval_rouge-2": 57.578572,
      "eval_rouge-l": 68.855234,
      "eval_runtime": 8.8261,
      "eval_samples_per_second": 5.665,
      "eval_steps_per_second": 0.453,
      "step": 1500
    },
    {
      "epoch": 0.86,
      "grad_norm": 6.043740272521973,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.3699,
      "step": 1510
    },
    {
      "epoch": 0.87,
      "grad_norm": 5.375128269195557,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.4097,
      "step": 1520
    },
    {
      "epoch": 0.87,
      "grad_norm": 2.990875482559204,
      "learning_rate": 2.45e-05,
      "loss": 0.3396,
      "step": 1530
    },
    {
      "epoch": 0.88,
      "grad_norm": 3.7649929523468018,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.3608,
      "step": 1540
    },
    {
      "epoch": 0.89,
      "grad_norm": 5.786558151245117,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.4323,
      "step": 1550
    },
    {
      "epoch": 0.89,
      "grad_norm": 3.873391628265381,
      "learning_rate": 2.4e-05,
      "loss": 0.2898,
      "step": 1560
    },
    {
      "epoch": 0.9,
      "grad_norm": 7.082992076873779,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.4022,
      "step": 1570
    },
    {
      "epoch": 0.9,
      "grad_norm": 5.341497898101807,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.4086,
      "step": 1580
    },
    {
      "epoch": 0.91,
      "grad_norm": 5.669996738433838,
      "learning_rate": 2.35e-05,
      "loss": 0.4205,
      "step": 1590
    },
    {
      "epoch": 0.91,
      "grad_norm": 7.8713483810424805,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.4249,
      "step": 1600
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.7251296043396,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.3944,
      "step": 1610
    },
    {
      "epoch": 0.93,
      "grad_norm": 4.129366397857666,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4494,
      "step": 1620
    },
    {
      "epoch": 0.93,
      "grad_norm": 4.458673477172852,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.3438,
      "step": 1630
    },
    {
      "epoch": 0.94,
      "grad_norm": 9.642346382141113,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.4194,
      "step": 1640
    },
    {
      "epoch": 0.94,
      "grad_norm": 5.362441062927246,
      "learning_rate": 2.25e-05,
      "loss": 0.3815,
      "step": 1650
    },
    {
      "epoch": 0.95,
      "grad_norm": 6.729567527770996,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.3902,
      "step": 1660
    },
    {
      "epoch": 0.95,
      "grad_norm": 4.401031017303467,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.3342,
      "step": 1670
    },
    {
      "epoch": 0.96,
      "grad_norm": 4.680820941925049,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.3816,
      "step": 1680
    },
    {
      "epoch": 0.97,
      "grad_norm": 5.267996311187744,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.3757,
      "step": 1690
    },
    {
      "epoch": 0.97,
      "grad_norm": 4.516189098358154,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.3703,
      "step": 1700
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.036468029022217,
      "learning_rate": 2.15e-05,
      "loss": 0.421,
      "step": 1710
    },
    {
      "epoch": 0.98,
      "grad_norm": 4.678486347198486,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.38,
      "step": 1720
    },
    {
      "epoch": 0.99,
      "grad_norm": 8.485503196716309,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.4177,
      "step": 1730
    },
    {
      "epoch": 0.99,
      "grad_norm": 8.317070960998535,
      "learning_rate": 2.1e-05,
      "loss": 0.4373,
      "step": 1740
    },
    {
      "epoch": 1.0,
      "grad_norm": 6.094749927520752,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.3057,
      "step": 1750
    },
    {
      "epoch": 1.01,
      "grad_norm": 9.617431640625,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.4135,
      "step": 1760
    },
    {
      "epoch": 1.01,
      "grad_norm": 7.768100738525391,
      "learning_rate": 2.05e-05,
      "loss": 0.3397,
      "step": 1770
    },
    {
      "epoch": 1.02,
      "grad_norm": 6.396751880645752,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.3007,
      "step": 1780
    },
    {
      "epoch": 1.02,
      "grad_norm": 4.4770636558532715,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.3778,
      "step": 1790
    },
    {
      "epoch": 1.03,
      "grad_norm": 5.286674499511719,
      "learning_rate": 2e-05,
      "loss": 0.4224,
      "step": 1800
    },
    {
      "epoch": 1.03,
      "grad_norm": 6.1207990646362305,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.3338,
      "step": 1810
    },
    {
      "epoch": 1.04,
      "grad_norm": 5.189561367034912,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.3804,
      "step": 1820
    },
    {
      "epoch": 1.05,
      "grad_norm": 5.8274383544921875,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.3707,
      "step": 1830
    },
    {
      "epoch": 1.05,
      "grad_norm": 4.70072078704834,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.4122,
      "step": 1840
    },
    {
      "epoch": 1.06,
      "grad_norm": 6.0286149978637695,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.3444,
      "step": 1850
    },
    {
      "epoch": 1.06,
      "grad_norm": 6.2949628829956055,
      "learning_rate": 1.9e-05,
      "loss": 0.4363,
      "step": 1860
    },
    {
      "epoch": 1.07,
      "grad_norm": 4.127385139465332,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.3515,
      "step": 1870
    },
    {
      "epoch": 1.07,
      "grad_norm": 5.061197757720947,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.3573,
      "step": 1880
    },
    {
      "epoch": 1.08,
      "grad_norm": 6.1776580810546875,
      "learning_rate": 1.85e-05,
      "loss": 0.3464,
      "step": 1890
    },
    {
      "epoch": 1.09,
      "grad_norm": 5.95277214050293,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.3729,
      "step": 1900
    },
    {
      "epoch": 1.09,
      "grad_norm": 6.247156143188477,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.3594,
      "step": 1910
    },
    {
      "epoch": 1.1,
      "grad_norm": 4.044057369232178,
      "learning_rate": 1.8e-05,
      "loss": 0.2999,
      "step": 1920
    },
    {
      "epoch": 1.1,
      "grad_norm": 7.91708517074585,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.3367,
      "step": 1930
    },
    {
      "epoch": 1.11,
      "grad_norm": 4.823880672454834,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.3828,
      "step": 1940
    },
    {
      "epoch": 1.11,
      "grad_norm": 10.791733741760254,
      "learning_rate": 1.75e-05,
      "loss": 0.3543,
      "step": 1950
    },
    {
      "epoch": 1.12,
      "grad_norm": 6.166304111480713,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.3605,
      "step": 1960
    },
    {
      "epoch": 1.13,
      "grad_norm": 6.8637590408325195,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.3553,
      "step": 1970
    },
    {
      "epoch": 1.13,
      "grad_norm": 7.745290279388428,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.3544,
      "step": 1980
    },
    {
      "epoch": 1.14,
      "grad_norm": 8.823111534118652,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.4121,
      "step": 1990
    },
    {
      "epoch": 1.14,
      "grad_norm": 4.295622825622559,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.3781,
      "step": 2000
    },
    {
      "epoch": 1.14,
      "eval_bleu-4": 0.6021443379037844,
      "eval_rouge-1": 79.27932799999999,
      "eval_rouge-2": 59.121294,
      "eval_rouge-l": 71.094276,
      "eval_runtime": 10.1899,
      "eval_samples_per_second": 4.907,
      "eval_steps_per_second": 0.393,
      "step": 2000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 2000,
  "total_flos": 2.805452827995341e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
